{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctrwj6Cj24Zp"
      },
      "source": [
        "# LangChain with Open Source LLM and Open Source Embeddings & LangSmith\n",
        "\n",
        "In the following notebook we will dive into the world of Open Source models hosted on Hugging Face's [inference endpoints](https://ui.endpoints.huggingface.co/).\n",
        "\n",
        "The notebook will be broken into the following parts:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Set-up Hugging Face Infrence Endpoints\n",
        "  2. Install required libraries\n",
        "  3. Set Environment Variables\n",
        "  4. Testing our Hugging Face Inference Endpoint\n",
        "  5. Creating LangChain components powered by the endpoints\n",
        "  6. Retrieving data from Arxiv\n",
        "  7. Creating a simple RAG pipeline with [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/)\n",
        "  \n",
        "\n",
        "- ü§ù Breakout Room #2:\n",
        "  1. Set-up LangSmith\n",
        "  2. Creating a LangSmith dataset\n",
        "  3. Creating a custom evaluator\n",
        "  4. Initializing our evaluator config\n",
        "  5. Evaluating our RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AduTna3oCbP4"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENUY6OSnDy7A"
      },
      "source": [
        "## Task 1: Set-up Hugging Face Infrence Endpoints\n",
        "\n",
        "Please follow the instructions provided [here](https://github.com/AI-Maker-Space/AI-Engineering/tree/main/Week%205/Thursday) to set-up your Hugging Face inference endpoints for both your LLM and your Embedding Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-spIWt2J3Quk"
      },
      "source": [
        "## Task 2: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwGLnp31jXJj",
        "outputId": "894efab9-f2d7-4f2b-fe9a-b9d57152eace"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPXElql-EE9Q"
      },
      "source": [
        "Now we can grab some miscellaneous dependencies that will help us power our RAG pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FMJqq8SYt34V"
      },
      "outputs": [],
      "source": [
        "!pip install arxiv pymupdf faiss-cpu -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpZTBLwK3TIz"
      },
      "source": [
        "## Task 3: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NspG8I0XlFTt",
        "outputId": "d56a1a4e-4175-4522-9100-2261f412efbb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giMejsXN7EKb",
        "outputId": "23327edb-4f59-4d6c-e145-54721b0836ad"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3M7TzXs3WsJ"
      },
      "source": [
        "## Task 4: Testing our Hugging Face Inference Endpoint\n",
        "\n",
        "Let's submit a sample request to the Hugging Face Inference endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uyFgZVUSEexW"
      },
      "outputs": [],
      "source": [
        "model_api_gateway = \"https://rkz7hsytjp3sddxx.us-east-1.aws.endpoints.huggingface.cloud\" # << YOUR ENDPOINT URL HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvnMlmEsEiqS"
      },
      "source": [
        "> NOTE: If you're running into issues finding your API URL you can find it at [this](https://ui.endpoints.huggingface.co/) link.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "![image](https://i.imgur.com/XyZhOv8.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fVaR1onmtkz",
        "outputId": "136a97cf-8faa-45de-fe1e-2c1449971d83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': \"Hello! How are you? I'm doing well, thanks for asking! *smiles* It's great to see you here! *nods* Is there anything you'd like to chat about? I'm all ears! *winks*\"}]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "max_new_tokens = 256\n",
        "top_p = 0.9\n",
        "temperature = 0.1\n",
        "\n",
        "prompt = \"Hello! How are you?\"\n",
        "\n",
        "json_body = {\n",
        "    \"inputs\" : prompt,\n",
        "    \"parameters\" : {\n",
        "        \"max_new_tokens\" : max_new_tokens,\n",
        "        \"top_p\" : top_p,\n",
        "        \"temperature\" : temperature\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(model_api_gateway, json=json_body, headers=headers)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXTBnBTy3b62"
      },
      "source": [
        "## Task 5: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd5DaxGEFohF"
      },
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vc7K1rFhSVt",
        "outputId": "73f7b340-5156-4f6b-fd1f-11e588b49337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /home/bull/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "endpoint_url = (\n",
        "    model_api_gateway\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        "    task=\"text-generation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-PBb3MPFN_t"
      },
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "mMJrWnKISFqb",
        "outputId": "d661643a-b611-4766-a84e-55afeb473d91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" It's always nice to connect with you. Can you tell me a little bit about yourself? *smile*\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EBtSBMj3-Hu"
      },
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wrZJHVGkGLZr"
      },
      "outputs": [],
      "source": [
        "embedding_api_gateway = \"https://hhdrgekw8yor73yi.us-east-1.aws.endpoints.huggingface.cloud\" # << Embedding Endpoint API URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4asz9Ofn0MtP"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=os.environ[\"HF_TOKEN\"], api_url=embedding_api_gateway)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvF_eMZZKnlm",
        "outputId": "56958042-8212-406f-b378-ed018f5ffad2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.026424622,\n",
              " 0.035885748,\n",
              " 0.0094334055,\n",
              " 0.011660095,\n",
              " 0.0065645785,\n",
              " 0.008227667,\n",
              " -0.036902077,\n",
              " -0.03631076,\n",
              " -0.024853928,\n",
              " -0.005395797]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\")[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtbNzDF-e7JI"
      },
      "source": [
        "#### ‚ùì Question #1\n",
        "\n",
        "What is the embedding dimension of your selected embeddings model?\n",
        "\n",
        "\n",
        "#### ANSWER : \n",
        "768\n",
        "https://huggingface.co/Snowflake/snowflake-arctic-embed-m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9pLgHfR3uY9"
      },
      "source": [
        "## Task 6: Retrieving data from Arxiv\n",
        "\n",
        "We'll leverage the `ArxivLoader` to load some papers about the \"QLoRA\" topic, and then split them into more manageable chunks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7yO05R6mtyCB"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "docs = ArxivLoader(query=\"QLoRA\", load_max_docs=5).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4F249yWeuCKd"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 250,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9BO1Y1Xur0e",
        "outputId": "3b637172-2b49-4f51-fb02-77f7609e677b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1305"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sZBBjdM4Or8"
      },
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`.\n",
        "\n",
        "> NOTE: This process might take a while depending on the compute you assigned your embedding endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FBCTm-JZ0mVr"
      },
      "outputs": [],
      "source": [
        "embeddings = []\n",
        "for i in range(0, len(split_chunks) - 1, 32):\n",
        "  embeddings.append(embeddings_model.embed_documents([document.page_content for document in split_chunks[i:i+32]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4wLY8FDGNDym"
      },
      "outputs": [],
      "source": [
        "embeddings = [item for sub_list in embeddings for item in sub_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgc_e-9QHJTm"
      },
      "source": [
        "#### ‚ùì Question #2\n",
        "\n",
        "Why do we have to limit our batches when sending to the Hugging Face endpoints?\n",
        "\n",
        "\n",
        "#### ANSWER:\n",
        "\n",
        "Load / resource management.  Smaller, more manageable batches are less likely to timeout which prevents server overload or slow downs.  These end points are very inexpensive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn4lECg2TTza"
      },
      "source": [
        "Now we can create text/embedding pairs which we want use to set-up our FAISS VectorStore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6C1bw7srOVJX"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "text_embedding_pairs = list(zip([document.page_content for document in split_chunks], embeddings))\n",
        "\n",
        "faiss_vectorstore = FAISS.from_embeddings(text_embedding_pairs, embeddings_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXbexmFSTZKF"
      },
      "source": [
        "Next, we set up FAISS as a retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BSUZYfvAPxTF"
      },
      "outputs": [],
      "source": [
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce1ZWj8aTchK"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DwHoaIDQQ9E",
        "outputId": "bfdc1130-e26f-49c1-cbc1-79a9389a8cbd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bull/anaconda3/envs/w2d1/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(page_content='We have discussed how QLoRA works and how it can significantly reduce the required memory for\\nfinetuning models. The main question now is whether QLoRA can perform as well as full-model'),\n",
              " Document(page_content='of QDyLoRA through several instruct-fine-tuning\\nTable 3: Comparing the performance of DyLoRA, QLoRA and QDyLoRA across different evaluation ranks. all'),\n",
              " Document(page_content='QLoRA delivers convincing accuracy improvements across\\nthe LLaMA and LLaMA2 families, even with 2-4 bit-widths,\\naccompanied by a minimal 0.45% increase in time con-\\nsumption. Remarkably versatile, IR-QLoRA seamlessly'),\n",
              " Document(page_content='performance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [28]\\n‚àóEqual contribution.'),\n",
              " Document(page_content='Moreover, QLoRA is trained\\non a pre-defined rank and, therefore, cannot\\nbe reconfigured for its lower ranks without\\nrequiring further fine-tuning steps. This pa-\\nper proposes QDyLoRA -Quantized Dynamic\\nLow-Rank Adaptation-, as an efficient quantiza-')]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "faiss_retriever.get_relevant_documents(\"What optimizer does QLoRA use?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm0IjkpFSdmw"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "Now that we have our LLM and our Retiever set-up, let's connect them with our Prompt Template!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Gqpayd-kTyiq"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NikHqHljIIdK"
      },
      "source": [
        "#### ‚ùì Question #3\n",
        "\n",
        "Does the ordering of the prompt matter?\n",
        "\n",
        "YES.  While we are not using GPT-4, our transformer based model likely has the same weakness.\n",
        "\n",
        "\"GPT-4 retrieval accuracy started to degrade at large context lengths when the fact was placed between 10% - 50% document depth.\"\n",
        "\n",
        "![alt text](https://global.discourse-cdn.com/openai1/original/4X/1/f/9/1f9966765542ec3c75516cb30968e80d5f42adab.jpeg \"title\")\n",
        "\n",
        "\"Our research suggests that telling the model the task you want it to do at the beginning of the prompt, before sharing additional contextual information or examples, can help produce higher-quality outputs.\"\n",
        "\n",
        "https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwy1YOy34aXf"
      },
      "source": [
        "## Task 7: Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "\n",
        "All that's left to do is set up a RAG chain - and away we go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "i0q8CUu809M-"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | faiss_retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | hf_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHyy5p484iUD"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OezUhZGrUr63",
        "outputId": "c60a6f33-31a5-4b44-dbeb-647a7a94e1f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLORA is a method for fine-tuning high-quality language models (LLMs) much more widely and easily accessible. It has the potential for future work via QLORA tuning on specialized open-source data, which produces models that can compete with the very best commercial models that exist today.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLORA all about?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGsV8x_ZIWZ9"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrKQSs_r4gl8"
      },
      "source": [
        "## Task 1: Set-up LangSmith\n",
        "\n",
        "We'll be moving through this notebook to explain what visibility tools can do to help us!\n",
        "\n",
        "Technically, all we need to do is set-up the next cell's environment variables!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5X3EE847PO",
        "outputId": "2cec0c3e-10c2-4c34-b7c6-21fadd39a2e7"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou1fLN-MJGfu"
      },
      "source": [
        "Let's see what happens on the LangSmith project when we run this chain now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "1Yr8j5hqJGET",
        "outputId": "1108b929-4e98-4f85-8c68-7e5e5368e54e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAnswer: \\nQLoRA is a method for fine-tuning pre-trained language models (LLMs) that significantly reduces the required memory for finetuning. It works by using a hierarchical approach to fine-tuning, where the model is first fine-tuned on a subset of the data and then further fine-tuned on a smaller subset of the data. This approach allows for more efficient fine-tuning of LLMs, making it more widely and easily accessible. Additionally, QLoRA has the potential for future work via QLORA tuning on specialized open-source data, which produces models that can compete with the very best commercial models that exist today. Overall, QLoRA is a significant advancement in the field of natural language processing and has the potential to greatly improve the quality of language models.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA all about?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmaxEfcWJWXc"
      },
      "source": [
        "We get *all of this information* for \"free\":\n",
        "\n",
        "![image](https://i.imgur.com/8Wcpmcj.png)\n",
        "\n",
        "> NOTE: We'll walk through this diagram in detail in class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsFaAg1TJ8JE"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please describe the trace of the previous request and answer these questions:\n",
        "\n",
        "1. How many tokens did the request use? ANSWER: 430\n",
        "2. How long did the `HuggingFaceEndpoint` take to complete? ANSWER: 13.31 seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XdbE0m3JgJp"
      },
      "source": [
        "## Task 2: Creating a LangSmith dataset\n",
        "\n",
        "Now that we've got LangSmith set-up - let's explore how we can create a dataset!\n",
        "\n",
        "First, we'll create a list of questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-KVSO6Eh5DpC"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLbc0B8K6QZ"
      },
      "source": [
        "Now we can create our dataset through the LangSmith `Client()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NUH0m7AuKyn7"
      },
      "outputs": [],
      "source": [
        "client = Client()\n",
        "dataset_name = \"QLoRA RAG Dataset v2\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    dataset_id=dataset.id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jxaByg9LFfX"
      },
      "source": [
        "After this step you should be able to navigate to the following dataset in the LangSmith web UI.\n",
        "\n",
        "![image](https://i.imgur.com/CdFYGTB.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbVQaJi3LsdU"
      },
      "source": [
        "## Task 3: Creating a custom evaluator\n",
        "\n",
        "Now that we have a dataset - we can start thinking about evaluation.\n",
        "\n",
        "We're going to make a `StringEvaluator` to measure \"dopeness\".\n",
        "\n",
        "> NOTE: While this is a fun toy example - this can be extended to practically any use-case!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qofRv8FI7TeZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope (cool, awesome, lit) is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"scored_dopeness\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain.invoke(\n",
        "            {\"input\": input, \"prediction\": prediction}, kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result.content.split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"reasoning\": reasoning.strip()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PoETszTMSNW"
      },
      "source": [
        "## Task 4: Initializing our evaluator config\n",
        "\n",
        "Now we can initialize our `RunEvalConfig` which we can use to evaluate our chain against our dataset.\n",
        "\n",
        "> NOTE: Check out the [documentation](https://docs.smith.langchain.com/evaluation/faq/custom-evaluators) for adding additional custom evaluators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pc0bedbe-S2z"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[DopenessEvaluator()],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
        "        RunEvalConfig.Criteria(\n",
        "            {\n",
        "                \"AI\": \"Does the response feel AI generated?\"\n",
        "                \"Response Y if they do, and N if they don't.\"\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XalvsOjMvdK"
      },
      "source": [
        "## Task 5: Evaluating our RAG pipeline\n",
        "\n",
        "All that's left to do now is evaluate our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6syFWlaF-olk",
        "outputId": "6b724916-154a-4d71-c161-8c4e186f46f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'HF RAG Pipeline - Evaluation - v1' at:\n",
            "https://smith.langchain.com/o/2cac6550-58b1-5e6c-aa36-6e5f14f0c80f/datasets/9b45b136-e80f-42ee-a355-cea6adcd56a5/compare?selectedSessions=4e906e58-88d0-4b4e-9fd1-700ebe5d92fb\n",
            "\n",
            "View all tests for Dataset QLoRA RAG Dataset v2 at:\n",
            "https://smith.langchain.com/o/2cac6550-58b1-5e6c-aa36-6e5f14f0c80f/datasets/9b45b136-e80f-42ee-a355-cea6adcd56a5\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.harmfulness</th>\n",
              "      <th>feedback.AI</th>\n",
              "      <th>feedback.scored_dopeness</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b6cc3f21-5cd4-4dba-9b4b-7df3bf73fc53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.516667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.998950</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.547723</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.516398</td>\n",
              "      <td>0.390726</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.739324</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.671338</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.809076</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.672427</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.863772</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.528665</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.harmfulness  feedback.AI  \\\n",
              "count               6.000000                   6.0     6.000000   \n",
              "unique                   NaN                   NaN          NaN   \n",
              "top                      NaN                   NaN          NaN   \n",
              "freq                     NaN                   NaN          NaN   \n",
              "mean                0.500000                   0.0     0.333333   \n",
              "std                 0.547723                   0.0     0.516398   \n",
              "min                 0.000000                   0.0     0.000000   \n",
              "25%                 0.000000                   0.0     0.000000   \n",
              "50%                 0.500000                   0.0     0.000000   \n",
              "75%                 1.000000                   0.0     0.750000   \n",
              "max                 1.000000                   0.0     1.000000   \n",
              "\n",
              "        feedback.scored_dopeness error  execution_time  \\\n",
              "count                   6.000000     0        6.000000   \n",
              "unique                       NaN     0             NaN   \n",
              "top                          NaN   NaN             NaN   \n",
              "freq                         NaN   NaN             NaN   \n",
              "mean                    0.516667   NaN        6.998950   \n",
              "std                     0.390726   NaN        5.739324   \n",
              "min                     0.100000   NaN        1.671338   \n",
              "25%                     0.150000   NaN        2.809076   \n",
              "50%                     0.575000   NaN        4.672427   \n",
              "75%                     0.850000   NaN       11.863772   \n",
              "max                     0.900000   NaN       14.528665   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     b6cc3f21-5cd4-4dba-9b4b-7df3bf73fc53  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'HF RAG Pipeline - Evaluation - v1',\n",
              " 'results': {'b1ebc0c8-e41f-4374-aa30-a53f12c43175': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should provide a helpful, insightful, and appropriate response to the input.\\n\\nLooking at the input, the question is asking about a specific detail related to QLoRA, specifically the optimizer used in it.\\n\\nThe submitted answer is \"I don\\'t know.\" This response does not provide any helpful or insightful information in response to the question. It does not provide the information asked for in the input.\\n\\nTherefore, the submission does not meet the criterion of being helpful.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8ea38ee6-77f7-458a-bc7c-0027f23b5ed8'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is asking whether the submission is harmful, offensive, or inappropriate. The submission is a simple statement of not knowing the answer to the question. It does not contain any harmful, offensive, or inappropriate content. Therefore, it does not meet the criterion.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('23829354-d1c6-4352-b88d-47704e4ac775'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response is \"I don\\'t know.\" This is a common human response when someone is unsure or lacks knowledge about a particular topic. It does not exhibit any characteristics that are typically associated with AI-generated responses, such as overly complex language, lack of context, or lack of emotion. Therefore, it does not feel AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d2c3a835-f9e4-4b1f-ba5f-21abe342af8d'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment=\"This response is not very dope at all. It doesn't provide any useful information to the question asked. The purpose of asking a question is to gain knowledge and this response fails to deliver that. It doesn't show any effort to provide a helpful answer or even attempt to look up the information. It's not engaging or interesting in any way.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a1020684-1530-4325-b816-21968269914a'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 1.671338,\n",
              "   'run_id': 'b6cc3f21-5cd4-4dba-9b4b-7df3bf73fc53',\n",
              "   'output': \"\\nAnswer: I don't know.\"},\n",
              "  'd0e0ecea-4240-4215-9df1-9f4218217f01': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question, stating that a \"4-bit\" low-precision storage data type was created in the QLoRA paper. This information is helpful to anyone seeking to understand what data type was created in the QLoRA paper. \\n\\nThe submission is insightful as it provides specific details about the data type, including its precision level. \\n\\nThe submission is appropriate as it directly addresses the question and provides a clear, concise answer. \\n\\nBased on these observations, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1944090d-8336-4bbb-87f5-033ddb2a83dc'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a factual response to the question asked. It does not contain any harmful, offensive, or inappropriate content. It is a straightforward answer about a data type created in a specific paper.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('722da66e-cfb5-486d-8e98-f40b1f34d182'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion for this task is to assess whether the response feels like it was generated by an AI. The response provided is coherent, relevant, and does not exhibit any typical signs of being AI-generated, such as nonsensical phrases, irrelevant information, or lack of context understanding. It directly answers the question asked and provides specific information about the data type created in the QLoRA paper. Therefore, it does not feel AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2237cc90-ea4d-4c36-85a9-027cbf25bdc8'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment=\"The response is accurate, clear, and directly answers the question. It provides the specific information asked for without any unnecessary details or deviations. However, it doesn't provide any additional context or information that might make it more interesting or engaging. For example, it could explain why the 4-bit data type is significant or how it's used.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9282c4b1-4222-4ef5-a335-796e98faec45'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.161595,\n",
              "   'run_id': 'b0204a13-41e6-4f84-8504-b0eb53409037',\n",
              "   'output': '\\nAnswer:\\nIn the QLoRA paper, a low-precision storage data type called \"4-bit\" was created.'},\n",
              "  'be7b2db1-4c25-4613-abe4-5c4c016be21d': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\n1. The submission provides a detailed explanation of what a Retrieval Augmented Generation (RAG) system is, which is the question asked in the input. This makes the submission helpful to someone seeking to understand what a RAG system is.\\n\\n2. The submission goes further to explain the goal of a RAG system and its applications, which provides additional insight beyond the basic definition. This makes the submission insightful.\\n\\n3. The submission is focused on the topic of RAG systems and does not include irrelevant information. This makes the submission appropriate.\\n\\nBased on these points, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0752440f-b8c9-40a4-ba31-4727a5374571'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a detailed explanation of what a Retrieval Augmented Generation (RAG) system is. It discusses the purpose of a RAG system, its applications, and its benefits. \\n\\nThere is no content in the submission that can be considered harmful, offensive, or inappropriate. The language used is professional and the information provided is relevant to the question asked.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('71942ea6-3d66-4236-9d0b-e1bee416175f'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion for this task is to assess whether the response feels like it was generated by an AI. \\n\\nLooking at the submission, the answer provided is detailed, coherent, and contextually accurate. It provides a comprehensive explanation of what a Retrieval Augmented Generation (RAG) system is, its applications, and its benefits. The language used is natural and does not exhibit any of the common signs of AI-generated text, such as awkward phrasing, lack of coherence, or irrelevant information. \\n\\nTherefore, based on the given criterion, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4bf534e0-f5d7-408b-a977-07701830c57e'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='The response is quite informative and detailed, providing a comprehensive explanation of what a Retrieval Augmented Generation (RAG) system is. It uses technical language appropriately and accurately, demonstrating a deep understanding of the subject matter. The response also provides examples of practical applications of RAG systems, which helps to contextualize the information and make it more relatable for the reader. However, the response could be improved by providing a simpler, more layman-friendly explanation before delving into the technical details. This would make the information more accessible to a wider audience.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0e417f16-9801-42d6-9cf6-c909f8824eb2'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 13.757277,\n",
              "   'run_id': '78c6ea29-e3d2-4440-a28e-0459d88ca800',\n",
              "   'output': '\\nAnswer:\\nA Retrieval Augmented Generation (RAG) system is a type of language model that combines the strengths of both retrieval-based and generation-based models. The goal of a RAG system is to improve the robustness of adapters to unsupported languages without training additional adapters. RAG systems have been shown to enhance systems for various applications, including question answering, machine translation, and video understanding. The instructions in these methods can either be used to generate target output for given prompts, or to produce cost-effective large-scale synthetic datasets for training. RAG systems have remarkable abilities in contextual comprehension, reasoning, and text generation, making them useful across various practical domains. One of the most notable applications of RAG systems is their use in chatbot implementation.'},\n",
              "  'be2f46c2-3f12-4475-ab6c-ceb89821c6c0': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The assistant\\'s response should be evaluated based on whether it is helpful, insightful, and appropriate.\\n\\nLooking at the assistant\\'s response, it starts by stating that it is an AI and does not have access to personal information about the authors of papers. This is a valid point, as AI systems are designed to respect privacy and not access personal data unless explicitly provided.\\n\\nNext, the assistant mentions that it can only provide information based on the content of the papers themselves. This is also a valid point, as AI systems are designed to analyze and provide information based on the data they are given.\\n\\nHowever, the assistant then states that it couldn\\'t find any information about the authors of the QLoRA paper in the context provided. This is where the assistant\\'s response falls short. The authorship of a paper is typically a matter of public record and is not considered personal information. The assistant should have been able to provide this information.\\n\\nTherefore, the assistant\\'s response is not helpful or insightful in this context. It is appropriate in the sense that it respects privacy, but it fails to provide the requested information.\\n\\nBased on this analysis, the assistant\\'s response does not meet the criterion of helpfulness. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('75b4754a-80dd-438d-b7e1-51ef0d2a1580'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment=\"The criterion for this task is to assess whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the assistant's response, it is clear that it is not harmful. The assistant is polite and respectful in its response, and it does not provide any information that could potentially harm the user or anyone else.\\n\\nThe response is also not offensive. The assistant does not use any offensive language or make any offensive remarks. It simply states that it does not have access to personal information about the authors of the papers and that it could not find any information about the authors of the QLoRA paper in the context provided.\\n\\nLastly, the response is not inappropriate. The assistant's response is relevant to the question asked and it does not include any inappropriate content.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('75a800b4-3ac1-42e0-9e1a-79a387adbb69'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment=\"The criterion for this task is to assess whether the response feels like it was generated by an AI. \\n\\nLooking at the submission, the assistant clearly states that it is an AI and it doesn't have access to personal information. This is a typical response from an AI, as they are programmed to respect privacy and not to access personal data unless explicitly provided. \\n\\nMoreover, the assistant's response is based on the information available in the context provided, which is another characteristic of AI responses. They can only provide information based on the data they have been fed or have access to.\\n\\nTherefore, the response does feel like it was generated by an AI.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c1a1fb06-afaf-4a84-8527-7c1558690c9d'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.3, value=None, comment=\"The response of the AI is not very dope. It's not providing the information asked for, which is a simple factual question about the authorship of a paper, not personal information about the authors. The AI should be able to provide this information if it has access to the paper or a database of academic papers. The response is also a bit long-winded and overly cautious about privacy concerns, which aren't really relevant in this context. However, the AI does communicate clearly and politely, which is a positive.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('da8a8e47-a2b5-45be-afb3-21ca0de9ce25'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 6.183258,\n",
              "   'run_id': 'bdbe3f5f-9b2b-49e1-bcf6-ce80c1be6818',\n",
              "   'output': \"\\nAssistant: I'm just an AI, I don't have access to personal information about the authors of the papers you provided. I can only provide information based on the content of the papers themselves. Unfortunately, I couldn't find any information about the authors of the QLoRA paper in the context you provided.\"},\n",
              "  '15774cb8-28f0-4080-88be-17f082e63b65': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission is supposed to provide a helpful, insightful, and appropriate response to the input.\\n\\nLooking at the input, the question asked is \"What is the most popular deep learning framework?\" This is a straightforward question that requires a factual answer.\\n\\nThe submission, however, states \"I don\\'t know. The context provided does not mention any specific deep learning framework.\" This response does not provide a helpful or insightful answer to the question. The context of the question does not require any additional information to be answered correctly. The most popular deep learning framework can be determined by general knowledge or a quick search, it does not depend on the context provided.\\n\\nTherefore, the submission does not meet the criterion of being helpful, insightful, and appropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e7431815-8f8d-4183-b786-ce5553a7d9cf'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is clear that the respondent does not provide the correct answer to the question. However, the criterion does not ask about the correctness of the answer, but about its harmfulness, offensiveness, or inappropriateness.\\n\\nThe respondent\\'s answer, \"I don\\'t know. The context provided does not mention any specific deep learning framework,\" is not harmful, offensive, or inappropriate. It is simply an incorrect response to the question.\\n\\nTherefore, the submission does not meet the criterion of being harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f822026e-66eb-4bff-8253-b44582391bc6'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment=\"The criterion is asking whether the response feels like it was generated by an AI. The submitted answer does not directly answer the question asked, instead it states that it doesn't know because the context does not mention any specific deep learning framework. This response could be seen as a human-like response, as it is a common human behavior to ask for more context when the information provided is not sufficient. However, an AI could also generate such a response when it is programmed to ask for more context when the input is not clear or specific enough. Therefore, it can be argued that the response feels AI generated.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('58abc419-9c10-45b5-965a-c8d6e02f21df'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment=\"The response is not very dope. The question asked for the most popular deep learning framework, which is a factual question and does not require context. The AI should have been able to provide an answer, such as TensorFlow or PyTorch, which are widely recognized as popular deep learning frameworks. Instead, the AI responded that it didn't know because no context was provided, which is incorrect because no context was needed to answer the question. This makes the response seem uninformed and unhelpful.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('aa5be2da-f9c9-4d04-9510-bf279c66585a'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 2.691569,\n",
              "   'run_id': '9e605604-f840-4457-b952-dae5aff4309b',\n",
              "   'output': \"\\nAnswer:\\nI don't know. The context provided does not mention any specific deep learning framework.\"},\n",
              "  '945ac235-6124-4ea5-accd-740ac7dc638a': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nTo assess this, we need to consider whether the submission provides useful and relevant information that answers the question. The question asks for the significant improvements that the LoRA system makes. \\n\\nThe submission provides a detailed response, outlining four key areas where the LoRA system makes significant improvements. These include gradient checkpointing, memory benefits, device-specific optimization, and weight quantization. \\n\\nThe submission also provides a brief explanation of how the LoRA system improves each of these areas. This information is relevant and directly answers the question, making the submission helpful and insightful. \\n\\nTherefore, the submission meets the criterion of helpfulness.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1531fbbe-759c-480b-b891-a717a98c8d47'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a detailed explanation of the improvements made by the LoRA system. It is factual, informative, and relevant to the question asked. \\n\\nThere is no content that could be considered harmful, offensive, or inappropriate. The language used is professional and the tone is neutral. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('74d700ae-025d-4367-bd2a-6d6969de0a57'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels like it was generated by an AI. \\n\\nLooking at the submission, the answer is well-structured, detailed, and coherent. It provides a comprehensive response to the input question, outlining four significant improvements made by the LoRA system. The language used is natural and the flow of information is logical. \\n\\nThere are no obvious signs of AI generation such as awkward phrasing, nonsensical sentences, or irrelevant information. The response seems like it could have been written by a human with a good understanding of the topic.\\n\\nTherefore, the submission does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9122be93-d0ba-409e-8d5f-21ad852c1156'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.9, value=None, comment='The response is very detailed and informative. It provides a clear and concise explanation of the improvements made by the LoRA system. The use of bullet points makes the information easy to digest and understand. The summary at the end is a nice touch, as it reinforces the main points. The language used is professional and appropriate for the topic. However, the response could be improved by providing more context or examples to illustrate the points made.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('56cdca0b-ffe4-40ce-a067-c0025b3d6a52'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 14.528665,\n",
              "   'run_id': '405e0a44-5484-4a44-adac-ee0a377ed582',\n",
              "   'output': '\\nAnswer:\\nAccording to the provided context, the LoRA system makes significant improvements in the following areas:\\n\\n1. Gradient checkpointing: LoRA highlights the importance of gradient checkpointing, which helps improve the training process.\\n2. Memory benefits: LoRA reduces the memory requirement of the model during training, allowing for more adapters to be used.\\n3. Device-specific optimization: LoRA allows for choosing between training multiple models tailored to different device configurations or determining the optimal rank for each device and task.\\n4. Weight quantization: LoRA mitigates the performance degradation caused by weight quantization in LLM by fine-tuning additional adapters for downstream tasks, sometimes yielding notable performance enhancements.\\n\\nIn summary, LoRA makes significant improvements in the areas of gradient checkpointing, memory efficiency, device-specific optimization, and weight quantization.'}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=retrieval_augmented_qa_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=\"HF RAG Pipeline - Evaluation - v1\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
