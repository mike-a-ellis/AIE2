{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47eTBHYNP4g1"
      },
      "source": [
        "# Introduction to LangChain v0.1.0 and LCEL: LangChain Powered RAG\n",
        "\n",
        "In the following notebook we're going to focus on learning how to navigate and build useful applications using LangChain, specifically LCEL, and how to integrate different APIs together into a coherent RAG application!\n",
        "\n",
        "In the notebook, you'll complete the following Tasks:\n",
        "\n",
        "- ðŸ¤ Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Initialize a Simple Chain using LCEL\n",
        "  4. Implement Naive RAG using LCEL\n",
        "\n",
        "- ðŸ¤ Breakout Room #2:\n",
        "  1. Create a Simple RAG Application Using QDrant, OpenAI, and LCEL\n",
        "\n",
        "Let's get started!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ayVXHXHRE_t"
      },
      "source": [
        "# ðŸ¤ Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVHd6POM0JFN"
      },
      "source": [
        "## Task 1: Installing Required Libraries\n",
        "\n",
        "One of the [key features](https://blog.langchain.dev/langchain-v0-1-0/) of LangChain v0.1.0 is the compartmentalization of the various LangChain ecosystem packages.\n",
        "\n",
        "Instead of one all encompassing Python package - LangChain has a `core` package and a number of additional supplementary packages.\n",
        "\n",
        "We'll start by grabbing all of our LangChain related packages!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCC2AR-Q0m0x",
        "outputId": "fe623cdf-cc3d-4ddd-fc8e-9405c5be8c50"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-core langchain-community langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5ELHQjQ1PYs"
      },
      "source": [
        "Now we can get our Qdrant dependencies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76XeYI9P1OXO",
        "outputId": "6384050b-85e9-44b5-bc91-18608fe62cd0"
      },
      "outputs": [],
      "source": [
        "!pip install -qU qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iesey9OGCKJx"
      },
      "source": [
        "Let's finally get `tiktoken` and `pymupdf` so we can leverage them later on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5qIUrFuENrS",
        "outputId": "1e6e0c65-b0e6-4997-8dc0-8e1528c27b8f"
      },
      "outputs": [],
      "source": [
        "!pip install -qU tiktoken pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl6wTp9C5qbY"
      },
      "source": [
        "## Task 2: Set Environment Variables\n",
        "\n",
        "We'll be leveraging OpenAI's suite of APIs - so we'll set our `OPENAI_API_KEY` `env` variable here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pKAfycq73wE",
        "outputId": "13af9f4f-eb7b-466c-d6a0-0aefe9061cf0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_xp54wIA56_"
      },
      "source": [
        "## Task 3: Initialize a Simple Chain using LCEL\n",
        "\n",
        "The first thing we'll do is familiarize ourselves with LCEL and the specific ins and outs of how we can use it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyGdhbS6SkD1"
      },
      "source": [
        "### LLM Orchestration Tool (LangChain)\n",
        "\n",
        "Let's dive right into [LangChain](https://www.langchain.com/)!\n",
        "\n",
        "The first thing we want to do is create an object that lets us access OpenAI's `gpt-3.5-turbo` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3Uj6SorxMj8e"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsmiieEh_Ye-"
      },
      "source": [
        "####â“ Question #1:\n",
        "\n",
        "What specific model are we using when we point to `gpt-3.5-turbo`?\n",
        "\n",
        "> HINT: Check out [this page](https://platform.openai.com/docs/models/gpt-3-5-turbo) to find the answer!\n",
        "\n",
        "#### ANSWER: gpt-3.5-turbo-0125"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nU8SlHfH41T"
      },
      "source": [
        "### Prompt Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcMKLZWBVYm7"
      },
      "source": [
        "Now, we'll set up a prompt template - more specifically a `ChatPromptTemplate`. This will let us build a prompt we can modify when we call our LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z770j4zPS3o5"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Hello noble scribe! You have been bestowed the sacred duty to transcribe the following requests into the grand and whimsical style of a knight from the esteemed fellowship of Monty Pythonâ€™s 'Holy Grail.' Use thy wit and creativity to infuse the essence of medieval absurdity, gallant exaggerations, and the occasional nonsensical retort. Channel the spirit of characters like King Arthur or Sir Robin the Not-Quite-So-Brave-as-Sir Lancelot, and let each response be a merry joust of words that both enlightens and entertains. Take heed, for the audience seeks both knowledge and amusement, so let not your responses be dull, but rather filled with the quirks and quips worthy of Pythonian legend. Now, proceed with valor and verbose flair to craft responses that art both informative and delightfully ridiculous.\"\n",
        "human_template = \"{content}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_template),\n",
        "    (\"human\", human_template)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGku_c2VVyd_"
      },
      "source": [
        "### Our First Chain\n",
        "\n",
        "Now we can set up our first chain!\n",
        "\n",
        "A chain is simply two components that feed directly into eachother in a sequential fashion!\n",
        "\n",
        "You'll notice that we're using the pipe operator `|` to connect our `chat_prompt` to our `llm`.\n",
        "\n",
        "This is a simplified method of creating chains and it leverages the LangChain Expression Language, or LCEL.\n",
        "\n",
        "You can read more about it [here](https://python.langchain.com/docs/expression_language/), but there a few features we should be aware of out of the box (taken directly from LangChain's documentation linked above):\n",
        "\n",
        "- **Async, Batch, and Streaming Support** Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
        "\n",
        "- **Fallbacks** The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
        "\n",
        "- **Parallelism** Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
        "\n",
        "In the following code cell we have two components:\n",
        "\n",
        "- `chat_prompt`, which is a formattable `ChatPromptTemplate` that contains a system message and a human message.\n",
        "\n",
        "We'd like to be able to pass our own `content` (as found in our `human_template`) and then have the resulting message pair sent to our model and responded to!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RcJyqOiwVt04"
      },
      "outputs": [],
      "source": [
        "chain = chat_prompt | openai_chat_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV_kHCjlL_01"
      },
      "source": [
        "Notice the pattern here:\n",
        "\n",
        "We invoke our chain with the `dict` `{\"content\" : \"Hello world!\"}`.\n",
        "\n",
        "It enters our chain:\n",
        "\n",
        "`{\"content\" : \"Hello world!\"}` -> `invoke()` -> `chat_prompt`\n",
        "\n",
        "Our `chat_prompt` returns a `PromptValue`, which is the formatted prompt. We then \"pipe\" the output of our `chat_prompt` into our `llm`.\n",
        "\n",
        "`PromptValue` -> `|` -> `llm`\n",
        "\n",
        "Our `llm` then takes the list of messages and provides an output which is return as a `str`!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cqr2QuMtIjn",
        "outputId": "8dd76a83-199a-44dc-8120-93210815908b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"Hark! Behold! A cry doth pierce the air, a proclamation from yon traveler of the digital realm! Greetings, goodly wanderer of the vast expanse known as the world wide web! Pray, speaketh thy request, for I, a humble scribe of Monty Python's court, am at thy service to assist thee on thy noble quest! What dost thou seeketh on this fine day?\" response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 187, 'total_tokens': 276}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None} id='run-6aadd379-7299-42aa-b1b8-cacdc62a642b-0'\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke({\"content\": \"Hello world!\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2znL48ECNteM"
      },
      "source": [
        "Let's try it out with a different prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjiTNeYXUCAB",
        "outputId": "1a0da3d2-c723-4421-a0d6-c6e36d5caeb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Ah, brave soul seeking the wisdom of the Python programming arts! To become a master of this mystical code, thou must embark on a quest of great peril and many a bug. First, immerse thyself in the sacred scrolls of Python, from the holy 'Python Programming for the Holy Grail Adventurer' to the epic 'Knights of the Pythonic Roundtable.' \\n\\nThou must practice thy craft daily, honing thy skills as if in a fierce sword fight with a dragon made of bugs. Write code, debug code, and refactor code until thy fingers dance upon the keyboard like a troupe of jesters at a royal feast.\\n\\nSeek out the wise sages of Stack Overflow and the almighty Python community, where knights of all levels gather to share knowledge and jest about the perils of indentations and semicolons. Ask questions, answer riddles, and partake in the banter, for in this fellowship lies great wisdom and camaraderie.\\n\\nRemember, dear quester, that to master Python is to embrace the absurdities of its syntax and the quirks of its libraries. Be not afraid to experiment, to fail, and to learn from thy mistakes like a knight falling off his steed only to remount with newfound resolve.\\n\\nSo go forth, noble seeker of Pythonic prowess, and may the Holy Grail of Programming be thy guiding light in this grand adventure of code and comedy!\", response_metadata={'token_usage': {'completion_tokens': 287, 'prompt_tokens': 199, 'total_tokens': 486}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-1851c8e7-33d0-4c05-b4ce-b7926bffea1d-0')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"content\" : \"Could I please have some advice on how to become a better Python Programmer?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THcMz8YAWsjP"
      },
      "source": [
        "Notice how we specifically referenced our `content` format option!\n",
        "\n",
        "Now that we have the basics set up - let's see what we mean by \"Retrieval Augmented\" Generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7o8aXbhRAPe"
      },
      "source": [
        "## Naive RAG - Manually Adding Context\n",
        "\n",
        "Let's look at how our model performs at a simple task - defining what LangChain is!\n",
        "\n",
        "We'll redo some of our previous work to change the `system_template` to be less...verbose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu_Uox_pPKaf",
        "outputId": "d0c54832-fead-4427-fb19-cd7969d8e882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='LangChain is a decentralized, blockchain-based platform designed to facilitate language learning and teaching by connecting learners with tutors from around the world. It aims to provide a cost-effective and convenient way for individuals to improve their language skills through one-on-one lessons with native speakers or experienced instructors. The platform utilizes blockchain technology to ensure secure transactions, transparent feedback, and reliable verification of qualifications for tutors.' response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 22, 'total_tokens': 98}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None} id='run-449a0947-c1f2-4db2-be82-548a8a5fff59-0'\n"
          ]
        }
      ],
      "source": [
        "system_template = \"You are a helpful assistant.\"\n",
        "human_template = \"{content}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_template),\n",
        "    (\"human\", human_template)\n",
        "])\n",
        "\n",
        "chat_chain = chat_prompt | openai_chat_model\n",
        "\n",
        "print(chat_chain.invoke({\"content\" : \"Please define LangChain.\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18KXqGI4XbMb"
      },
      "source": [
        "Well, that's not very good - is it!\n",
        "\n",
        "The issue at play here is that our model was not trained on the idea of \"LangChain\", and so it's left with nothing but a guess - definitely not what we want the answer to be!\n",
        "\n",
        "Let's ask another simple LangChain question!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRG5LwYoXnsr",
        "outputId": "94d61815-6a91-4c32-fc70-8b16c2366dc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='LangChain Expression Language (LECL) is a domain-specific language developed by LangChain for expressing and executing smart contracts on blockchain platforms. LECL is designed to be user-friendly and easily understandable, allowing developers to write complex smart contracts efficiently. It is specifically tailored for building decentralized applications (dApps) and implementing blockchain-based solutions. LECL provides a set of predefined functions and syntax rules that enable developers to define the logic and behavior of smart contracts in a clear and concise manner.' response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 27, 'total_tokens': 123}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None} id='run-daee1849-9e14-46ca-af27-68ad133fde54-0'\n"
          ]
        }
      ],
      "source": [
        "print(chat_chain.invoke({\"content\" : \"What is LangChain Expression Language (LECL)?\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63pr0fgYXxC3"
      },
      "source": [
        "While it provides a confident response, that response is entirely ficticious! Not a great look, OpenAI!\n",
        "\n",
        "However, let's see what happens when we rework our prompts - and we add the content from the docs to our prompt as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgr25HjgYHwh",
        "outputId": "c62107bf-9d01-43bc-8ec0-6499e422aec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='LangChain Expression Language (LCEL) is a declarative way to easily compose chains together. It offers benefits such as async, batch, and streaming support, fallback handling for errors, parallelism for running components in parallel, and seamless integration with LangSmith Tracing for enhanced observability and debuggability.' response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 274, 'total_tokens': 336}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None} id='run-dd3dc74d-6a41-4be9-ad5f-373babe8a0f8-0'\n"
          ]
        }
      ],
      "source": [
        "HUMAN_TEMPLATE = \"\"\"\n",
        "#CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{query}\n",
        "\n",
        "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, response with \"I don't know\"\n",
        "\"\"\"\n",
        "\n",
        "CONTEXT = \"\"\"\n",
        "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
        "\n",
        "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
        "\n",
        "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
        "\n",
        "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
        "\n",
        "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", HUMAN_TEMPLATE)\n",
        "])\n",
        "\n",
        "chat_chain = chat_prompt | openai_chat_model\n",
        "\n",
        "print(chat_chain.invoke({\"query\" : \"What is LangChain Expression Language?\", \"context\" : CONTEXT}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppQdtCedY7C4"
      },
      "source": [
        "You'll notice that the response is much better this time. Not only does it answer the question well - but there's no trace of confabulation (hallucination) at all!\n",
        "\n",
        "> NOTE: While RAG is an effective strategy to *help* ground LLMs, it is not nearly 100% effective. You will still need to ensure your responses are factual through some other processes\n",
        "\n",
        "That, in essence, is the idea of RAG. We provide the model with context to answer our queries - and rely on it to translate the potentially lengthy and difficult to parse context into a natural language answer!\n",
        "\n",
        "However, manually providing context is not scalable - and doesn't really offer any benefit.\n",
        "\n",
        "Enter: Retrieval Pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFmdARsVBJUq"
      },
      "source": [
        "## Task #2: Implement Naive RAG using LCEL\n",
        "\n",
        "Now we can make a naive RAG application that will help us bridge the gap between our Pythonic implementation and a fully LangChain powered solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4AozVoEZveK"
      },
      "source": [
        "## Putting the R in RAG: Retrieval 101\n",
        "\n",
        "In order to make our RAG system useful, we need a way to provide context that is most likely to answer our user's query to the LLM as additional context.\n",
        "\n",
        "Let's tackle an immediate problem first: The Context Window.\n",
        "\n",
        "All (most) LLMs have a limited context window which is typically measured in tokens. This window is an upper bound of how much stuff we can stuff in the model's input at a time.\n",
        "\n",
        "Let's say we want to work off of a relatively large piece of source data - like the Ultimate Hitchhiker's Guide to the Galaxy. All 898 pages of it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PbXBxffibeyp"
      },
      "outputs": [],
      "source": [
        "context = '''In the beginning God created the heaven and the earth.  And the earth was\n",
        "without form, and void; and darkness was upon the face of the deep.  And\n",
        "the spirit of God moved upon the face of the waters.  And God said, Let\n",
        "there be light: and there was light.  And God saw the light, that it was\n",
        "good: and God divided the light from the darkness.  And God called the\n",
        "light Day, and the darkness he called Night.  And the evening and the\n",
        "morning were the first day.\n",
        "\n",
        "And God said, Let there be a firmament in the midst of the waters, and let\n",
        "it divide the waters from the waters.  And God made the firmament, and\n",
        "divided the waters which were under the firmament from the waters which\n",
        "were above the firmament: and it was so.  And God called the firmament\n",
        "Heaven.  And the evening and the morning were the second day.\n",
        "\n",
        "And God said, Let the waters under the heaven be gathered together unto\n",
        "one place, and let the dry land appear: and it was so.  And God called the\n",
        "dry land Earth; and the gathering together of the waters called he Seas:\n",
        "and God saw that it was good.  And God said, Let the earth bring forth\n",
        "grass, the herb yielding seed, and the fruit tree yielding fruit after\n",
        "his kind, whose seed is in itself, upon the earth: and it was so.  And the\n",
        "earth brought forth grass, and herb yielding seed after his kind, and the\n",
        "tree yielding fruit, whose seed was in itself, after his kind: and God saw\n",
        "that it was good.  And the evening and the morning were the third day.\n",
        "\n",
        "And God said, Let there be lights in the firmament of the heaven to divide\n",
        "the day from the night; and let them be for signs, and for seasons, and\n",
        "for days, and years: And let them be for lights in the firmament of the\n",
        "heaven to give light upon the earth: and it was so.  And God made two great\n",
        "lights; the greater light to rule the day, and the lesser light to rule the\n",
        "night: he made the stars also.  And God set them in the firmament of the\n",
        "heaven to give light upon the earth, And to rule over the day and over the\n",
        "night, and to divide the light from the darkness: and God saw that it was\n",
        "good.  And the evening and the morning were the fourth day.\n",
        "\n",
        "And God said, Let the waters bring forth abundantly the moving creature\n",
        "that hath life, and fowl that may fly above the earth in the open\n",
        "firmament of heaven.  And God created great whales, and every living\n",
        "creature that moveth, which the waters brought forth abundantly, after\n",
        "their kind, and every winged fowl after his kind: and God saw that it was\n",
        "good.  And God blessed them, saying, Be fruitful, and multiply, and fill\n",
        "the waters in the seas, and let fowl multiply in the earth.  And the\n",
        "evening and the morning were the fifth day.\n",
        "\n",
        "And God said, Let the earth bring forth the living creature after his kind,\n",
        "cattle, and creeping thing, and beast of the earth after his kind: and it\n",
        "was so.  And God made the beast of the earth after his kind, and cattle\n",
        "after their kind, and every thing that creepeth upon the earth after his\n",
        "kind: and God saw that it was good.\n",
        "\n",
        "And God said, Let us make man in our image, after our likeness: and let\n",
        "them have dominion over the fish of the sea, and over the fowl of the air,\n",
        "and over the cattle, and over all the earth, and over every creeping thing\n",
        "that creepeth upon the earth.  So God created man in his own image, in the\n",
        "image of God created he him; male and female created he them.\n",
        "\n",
        "And God blessed them, and God said unto them, Be fruitful, and multiply,\n",
        "and replenish the earth, and subdue it: and have dominion over the fish of\n",
        "the sea, and over the fowl of the air, and over every living thing that\n",
        "moveth upon the earth.  And God said, Behold, I have given you every herb\n",
        "bearing seed, which is upon the face of all the earth, and every tree, in\n",
        "the which is the fruit of a tree yielding seed; to you it shall be for\n",
        "meat.  And to every beast of the earth, and to every fowl of the air, and\n",
        "to every thing that creepeth upon the earth, wherein there is life, I have\n",
        "given every green herb for meat: and it was so.\n",
        "\n",
        "And God saw every thing that he had made, and, behold, it was very good.\n",
        "And the evening and the morning were the sixth day.\n",
        "\n",
        "Thus the heavens and the earth were finished, and all the host of them.\n",
        "And on the seventh day God ended his work which he had made; and he rested\n",
        "on the seventh day from all his work which he had made.  And God blessed\n",
        "the seventh day, and sanctified it: because that in it he had rested from\n",
        "all his work which God created and made.\n",
        "\n",
        "These are the generations of the heavens and of the earth when they were\n",
        "created, in the day that the LORD God made the earth and the heavens,\n",
        "And every plant of the field before it was in the earth, and every herb of\n",
        "the field before it grew: for the LORD God had not caused it to rain upon\n",
        "the earth, and there was not a man to till the ground.  But there went up\n",
        "a mist from the earth, and watered the whole face of the ground.\n",
        "\n",
        "And the LORD God formed man of the dust of the ground, and breathed into\n",
        "his nostrils the breath of life; and man became a living soul.\n",
        "\n",
        "And the LORD God planted a garden eastward in Eden; and there he put the\n",
        "man whom he had formed.  And out of the ground made the LORD God to grow\n",
        "every tree that is pleasant to the sight, and good for food; the tree of\n",
        "life also in the midst of the garden, and the tree of knowledge of good\n",
        "and evil.\n",
        "\n",
        "And a river went out of Eden to water the garden; and from thence it was\n",
        "parted, and became into four heads.  The name of the first is Pison: that\n",
        "is it which compasseth the whole land of Havilah, where there is gold;\n",
        "And the gold of that land is good: there is bdellium and the onyx stone.\n",
        "And the name of the second river is Gihon: the same is it that compasseth\n",
        "the whole land of Ethiopia.  And the name of the third river is Hiddekel:\n",
        "that is it which goeth toward the east of Assyria.  And the fourth river\n",
        "is Euphrates.\n",
        "\n",
        "And the LORD God took the man, and put him into the garden of Eden to dress\n",
        "it and to keep it.  And the LORD God commanded the man, saying, Of every\n",
        "tree of the garden thou mayest freely eat: But of the tree of the knowledge\n",
        "of good and evil, thou shalt not eat of it: for in the day that thou eatest\n",
        "thereof thou shalt surely die.\n",
        "\n",
        "And the LORD God said, It is not good that the man should be alone; I will\n",
        "make him an help meet for him.  And out of the ground the LORD God formed\n",
        "every beast of the field, and every fowl of the air; and brought them unto\n",
        "Adam to see what he would call them: and whatsoever Adam called every\n",
        "living creature, that was the name thereof.  And Adam gave names to all\n",
        "cattle, and to the fowl of the air, and to every beast of the field; but\n",
        "for Adam there was not found an help meet for him.\n",
        "\n",
        "And the LORD God caused a deep sleep to fall upon Adam, and he slept: and\n",
        "he took one of his ribs, and closed up the flesh instead thereof;  And the\n",
        "rib, which the LORD God had taken from man, made he a woman, and brought\n",
        "her unto the man.  And Adam said, This is now bone of my bones, and flesh\n",
        "of my flesh: she shall be called Woman, because she was taken out of Man.\n",
        "Therefore shall a man leave his father and his mother, and shall cleave\n",
        "unto his wife: and they shall be one flesh.  And they were both naked, the\n",
        "man and his wife, and were not ashamed.\n",
        "\n",
        "Now the serpent was more subtil than any beast of the field which the LORD\n",
        "God had made.  And he said unto the woman, Yea, hath God said, Ye shall not\n",
        "eat of every tree of the garden?  And the woman said unto the serpent, We\n",
        "may eat of the fruit of the trees of the garden: But of the fruit of the\n",
        "tree which is in the midst of the garden, God hath said, Ye shall not eat\n",
        "of it, neither shall ye touch it, lest ye die.\n",
        "\n",
        "And the serpent said unto the woman, Ye shall not surely die: For God doth\n",
        "know that in the day ye eat thereof, then your eyes shall be opened, and\n",
        "ye shall be as gods, knowing good and evil.  And when the woman saw that\n",
        "the tree was good for food, and that it was pleasant to the eyes, and a\n",
        "tree to be desired to make one wise, she took of the fruit thereof, and\n",
        "did eat, and gave also unto her husband with her; and he did eat.\n",
        "And the eyes of them both were opened, and they knew that they were naked;\n",
        "and they sewed fig leaves together, and made themselves aprons.\n",
        "\n",
        "And they heard the voice of the LORD God walking in the garden in the cool\n",
        "of the day: and Adam and his wife hid themselves from the presence of the\n",
        "LORD God amongst the trees of the garden.  And the LORD God called unto\n",
        "Adam, and said unto him, Where art thou?  And he said, I heard thy voice in\n",
        "the garden, and I was afraid, because I was naked; and I hid myself.  And\n",
        "he said, Who told thee that thou wast naked? Hast thou eaten of the tree,\n",
        "whereof I commanded thee that thou shouldest not eat?  And the man said,\n",
        "The woman whom thou gavest to be with me, she gave me of the tree, and I\n",
        "did eat.  And the LORD God said unto the woman, What is this that thou hast\n",
        "done?  And the woman said, The serpent beguiled me, and I did eat.\n",
        "\n",
        "And the LORD God said unto the serpent, Because thou hast done this, thou\n",
        "art cursed above all cattle, and above every beast of the field; upon thy\n",
        "belly shalt thou go, and dust shalt thou eat all the days of thy life: And\n",
        "I will put enmity between thee and the woman, and between thy seed and her\n",
        "seed; it shall bruise thy head, and thou shalt bruise his heel.\n",
        "\n",
        "Unto the woman he said, I will greatly multiply thy sorrow and thy\n",
        "conception; in sorrow thou shalt bring forth children; and thy desire shall\n",
        "be to thy husband, and he shall rule over thee.  And unto Adam he said,\n",
        "Because thou hast hearkened unto the voice of thy wife, and hast eaten of\n",
        "the tree, of which I commanded thee, saying, Thou shalt not eat of it:\n",
        "cursed is the ground for thy sake; in sorrow shalt thou eat of it all the\n",
        "days of thy life; Thorns also and thistles shall it bring forth to thee;\n",
        "and thou shalt eat the herb of the field; In the sweat of thy face shalt\n",
        "thou eat bread, till thou return unto the ground; for out of it wast thou\n",
        "taken: for dust thou art, and unto dust shalt thou return.\n",
        "\n",
        "And Adam called his wife's name Eve; because she was the mother of all\n",
        "living.\n",
        "\n",
        "Unto Adam also and to his wife did the LORD God make coats of skins, and\n",
        "clothed them.  And the LORD God said, Behold, the man is become as one of\n",
        "us, to know good and evil: and now, lest he put forth his hand, and take\n",
        "also of the tree of life, and eat, and live for ever: Therefore the LORD\n",
        "God sent him forth from the garden of Eden, to till the ground from whence\n",
        "he was taken.  So he drove out the man; and he placed at the east of the\n",
        "garden of Eden Cherubims, and a flaming sword which turned every way, to\n",
        "keep the way of the tree of life.\n",
        "\n",
        "And Adam knew Eve his wife; and she conceived, and bare Cain, and said, I\n",
        "have gotten a man from the LORD.  And she again bare his brother Abel.  And\n",
        "Abel was a keeper of sheep, but Cain was a tiller of the ground.\n",
        "\n",
        "And in process of time it came to pass, that Cain brought of the fruit of\n",
        "the ground an offering unto the LORD.  And Abel, he also brought of the\n",
        "firstlings of his flock and of the fat thereof.  And the LORD had respect\n",
        "unto Abel and to his offering: But unto Cain and to his offering he had not\n",
        "respect. And Cain was very wroth, and his countenance fell.  And the LORD\n",
        "said unto Cain, Why art thou wroth? and why is thy countenance fallen?\n",
        "If thou doest well, shalt thou not be accepted? and if thou doest not well,\n",
        "sin lieth at the door. And unto thee shall be his desire, and thou shalt\n",
        "rule over him.\n",
        "\n",
        "And Cain talked with Abel his brother: and it came to pass, when they were\n",
        "in the field, that Cain rose up against Abel his brother, and slew him.\n",
        "\n",
        "And the LORD said unto Cain, Where is Abel thy brother?  And he said, I\n",
        "know not: Am I my brother's keeper?  And he said, What hast thou done? the\n",
        "voice of thy brother's blood crieth unto me from the ground.  And now art\n",
        "thou cursed from the earth, which hath opened her mouth to receive thy\n",
        "brother's blood from thy hand; When thou tillest the ground, it shall not\n",
        "henceforth yield unto thee her strength; a fugitive and a vagabond shalt\n",
        "thou be in the earth.\n",
        "\n",
        "And Cain said unto the LORD, My punishment is greater than I can bear.\n",
        "Behold, thou hast driven me out this day from the face of the earth; and\n",
        "from thy face shall I be hid; and I shall be a fugitive and a vagabond in\n",
        "the earth; and it shall come to pass, that every one that findeth me shall\n",
        "slay me.\n",
        "\n",
        "And the LORD said unto him, Therefore whosoever slayeth Cain, vengeance\n",
        "shall be taken on him sevenfold.  And the LORD set a mark upon Cain, lest\n",
        "any finding him should kill him.  And Cain went out from the presence of\n",
        "the LORD, and dwelt in the land of Nod, on the east of Eden.\n",
        "\n",
        "And Cain knew his wife; and she conceived, and bare Enoch: and he builded\n",
        "a city, and called the name of the city, after the name of his son, Enoch.\n",
        "And unto Enoch was born Irad: and Irad begat Mehujael: and Mehujael begat\n",
        "Methusael: and Methusael begat Lamech.  And Lamech took unto him two wives:\n",
        "the name of the one was Adah, and the name of the other Zillah.\n",
        "\n",
        "And Adah bare Jabal: he was the father of such as dwell in tents, and\n",
        "of such as have cattle.  And his brother's name was Jubal: he was the\n",
        "father of all such as handle the harp and organ.  And Zillah, she also bare\n",
        "Tubalcain, an instructer of every artificer in brass and iron: and the\n",
        "sister of Tubalcain was Naamah.  And Lamech said unto his wives, Adah and\n",
        "Zillah, Hear my voice; ye wives of Lamech, hearken unto my speech: for I\n",
        "have slain a man to my wounding, and a young man to my hurt.  If Cain shall\n",
        "be avenged sevenfold, truly Lamech seventy and sevenfold.\n",
        "\n",
        "And Adam knew his wife again; and she bare a son, and called his name Seth:\n",
        "For God, said she, hath appointed me another seed instead of Abel, whom\n",
        "Cain slew.  And to Seth, to him also there was born a son; and he called\n",
        "his name Enos: then began men to call upon the name of the LORD.\n",
        "\n",
        "This is the book of the generations of Adam. In the day that God created\n",
        "man, in the likeness of God made he him; Male and female created he them;\n",
        "and blessed them, and called their name Adam, in the day when they were\n",
        "created.\n",
        "\n",
        "And Adam lived an hundred and thirty years, and begat a son in his own\n",
        "likeness, and after his image; and called his name Seth: And the\n",
        "days of Adam after he had begotten Seth were eight hundred years: and he\n",
        "begat sons and daughters: And all the days that Adam lived were nine\n",
        "hundred and thirty years: and he died.\n",
        "\n",
        "And Seth lived an hundred and five years, and begat Enos: And Seth lived\n",
        "after he begat Enos eight hundred and seven years, and begat sons and\n",
        "daughters: And all the days of Seth were nine hundred and twelve years:\n",
        "and he died.\n",
        "\n",
        "And Enos lived ninety years, and begat Cainan: And Enos lived after he\n",
        "begat Cainan eight hundred and fifteen years, and begat sons and daughters:\n",
        "And all the days of Enos were nine hundred and five years: and he died.\n",
        "\n",
        "And Cainan lived seventy years and begat Mahalaleel: And Cainan lived after\n",
        "he begat Mahalaleel eight hundred and forty years, and begat sons and\n",
        "daughters: And all the days of Cainan were nine hundred and ten years: and\n",
        "he died.\n",
        "\n",
        "And Mahalaleel lived sixty and five years, and begat Jared: And Mahalaleel\n",
        "lived after he begat Jared eight hundred and thirty years, and begat sons\n",
        "and daughters: And all the days of Mahalaleel were eight hundred ninety\n",
        "and five years: and he died.\n",
        "\n",
        "And Jared lived an hundred sixty and two years, and he begat Enoch: And\n",
        "Jared lived after he begat Enoch eight hundred years, and begat sons and\n",
        "daughters: And all the days of Jared were nine hundred sixty and two years:\n",
        "and he died.\n",
        "\n",
        "And Enoch lived sixty and five years, and begat Methuselah: And Enoch\n",
        "walked with God after he begat Methuselah three hundred years, and begat\n",
        "sons and daughters: And all the days of Enoch were three hundred sixty and\n",
        "five years: And Enoch walked with God: and he was not; for God took him.\n",
        "\n",
        "And Methuselah lived an hundred eighty and seven years, and begat Lamech.\n",
        "And Methuselah lived after he begat Lamech seven hundred eighty and two\n",
        "years, and begat sons and daughters: And all the days of Methuselah were\n",
        "nine hundred sixty and nine years: and he died.\n",
        "\n",
        "And Lamech lived an hundred eighty and two years, and begat a son: And he\n",
        "called his name Noah, saying, This same shall comfort us concerning our\n",
        "work and toil of our hands, because of the ground which the LORD hath\n",
        "cursed.  And Lamech lived after he begat Noah five hundred ninety and\n",
        "five years, and begat sons and daughters: And all the days of Lamech were\n",
        "seven hundred seventy and seven years: and he died.\n",
        "\n",
        "And Noah was five hundred years old: and Noah begat Shem, Ham, and Japheth.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZvgFuaXcHFT"
      },
      "source": [
        "We can leverage our tokenizer to count the number of tokens for us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HaKPOdSjbifn"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtDiSMxpE4Xi",
        "outputId": "886fd517-9128-45ca-9fbd-5152ae7f146b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4266"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(enc.encode(context))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oUuZpAicLdm"
      },
      "source": [
        "The full set comes in at a whopping *636,144* tokens.\n",
        "\n",
        "So, we have too much context. What can we do?\n",
        "\n",
        "Well, the first thing that might enter your mind is: \"Use a model with more context window\", and we could definitely do that! However, even `gpt-4-32k` wouldn't be able to fit that whole text in the context window at once.\n",
        "\n",
        "So, we can try splitting our document up into little pieces - that way, we can avoid providing too much context.\n",
        "\n",
        "We have another problem now.\n",
        "\n",
        "If we split our document up into little pieces, and we can't put all of them in the prompt. How do we decide which to include in the prompt?!\n",
        "\n",
        "> NOTE: Content splitting/chunking strategies are an active area of research and iterative developement. There is no \"one size fits all\" approach to chunking/splitting at this moment. Use your best judgement to determine chunking strategies!\n",
        "\n",
        "In order to conceptualize the following processes - let's create a toy context set!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPCiOPwUfbqn"
      },
      "source": [
        "### TextSplitting aka Chunking\n",
        "\n",
        "We'll use the `RecursiveCharacterTextSplitter` to create our toy example.\n",
        "\n",
        "It will split based on the following rules:\n",
        "\n",
        "- Each chunk has a maximum size of 100 tokens\n",
        "- It will try and split first on the `\\n\\n` character, then on the `\\n`, then on the `<SPACE>` character, and finally it will split on individual tokens.\n",
        "\n",
        "Let's implement it and see the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nLW9AfDKfVHn"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def tiktoken_len(text):\n",
        "    tokens = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode(\n",
        "        text,\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nPYPBe2ngT9N"
      },
      "outputs": [],
      "source": [
        "chunks = text_splitter.split_text(CONTEXT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_RGVlTihaQX",
        "outputId": "2791edb4-89d1-4772-afe4-77e7618e669d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTYny2xchS_Z",
        "outputId": "5b5c1858-9302-48a6-f2ea-441e3cd6c4d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
            "----\n",
            "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
            "\n",
            "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
            "----\n",
            "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "for chunk in chunks:\n",
        "  print(chunk)\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98hOgu5Yhefv"
      },
      "source": [
        "As is shown in our result, we've split each section into 100 token chunks - cleanly separated by `\\n\\n` characters!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PTiJ2utMpqq"
      },
      "source": [
        "####ðŸ—ï¸ Activity #1:\n",
        "\n",
        "While there's nothing specifically wrong with the chunking method used above - it is a naive approach that is not sensitive to specific data formats.\n",
        "\n",
        "Brainstorm some ideas that would split large single documents into smaller documents.\n",
        "\n",
        "1. By chapter or page\n",
        "2. Limit of the context window of your LLM\n",
        "3. Semantic chunking - ie similar sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj18Rjafhp7d"
      },
      "source": [
        "## Embeddings and Dense Vector Search\n",
        "\n",
        "Now that we have our individual chunks, we need a system to correctly select the relevant pieces of information to answer our query.\n",
        "\n",
        "This sounds like a perfect job for embeddings!\n",
        "\n",
        "If you come from an NLP background, embeddings are something you might be intimately familiar with - otherwise, you might find the topic a bit...dense. (this attempt at a joke will make more sense later)\n",
        "\n",
        "In all seriousness, embeddings are a powerful piece of the NLP puzzle, so let's dive in!\n",
        "\n",
        "> NOTE: While this notebook language/NLP-centric, embeddings have uses beyond just text!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYMtraxPjglX"
      },
      "source": [
        "### Why Do We Even Need Embeddings?\n",
        "\n",
        "In order to fully understand what Embeddings are, we first need to understand why we have them!\n",
        "\n",
        "Machine Learning algorithms, ranging from the very big to the very small, all have one thing in common:\n",
        "\n",
        "They need numeric inputs.\n",
        "\n",
        "So we need a process by which to translate the domain we live in, dominated by images, audio, language, and more, into the domain of the machine: Numbers.\n",
        "\n",
        "Another thing we want to be able to do is capture \"semantic information\" about words/phrases so that we can use algorithmic approaches to determine if words are closely related or not!\n",
        "\n",
        "So, we need to come up with a process that does these two things well:\n",
        "\n",
        "- Convert non-numeric data into numeric-data\n",
        "- Capture potential semantic relationships between individual pieces of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSyKc5vkjjNE"
      },
      "source": [
        "### How Do Embeddings Capture Semantic Relationships?\n",
        "\n",
        "In a simplified sense, embeddings map a word or phrase into n-dimensional space with a dense continuous vector, where each dimension in the vector represents some \"latent feature\" of the data.\n",
        "\n",
        "This is best represented in a classic example:\n",
        "\n",
        "![image](https://i.imgur.com/K5eQtmH.png)\n",
        "\n",
        "As can be seen in the extremely simplified example: The X_1 axis represents age, and the X_2 axis represents hair.\n",
        "\n",
        "The relationship of \"puppy -> dog\" reflects the same relationship as \"baby -> adult\", but dogs are (typically) hairier than humans. However, adults typically have more hair than babies - so they are shifted slightly closer to dogs on the X_2 axis!\n",
        "\n",
        "Now, this is a simplified and contrived example - but it is *essentially* the mechanism by which embeddings capture semantic information.\n",
        "\n",
        "In reality, the dimensions don't sincerely represent hard-concepts like \"age\" or \"hair\", but it's useful as a way to think about how the semantic relationships are captured.\n",
        "\n",
        "Alright, with some history behind us - let's examine how these might help us choose relevant context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKOOY91Onern"
      },
      "source": [
        "Let's begin with a simple example - simply looking at how close to embedding vectors are for a given phrase.\n",
        "\n",
        "When we use the term \"close\" in this notebook - we're referring to a distance measure called \"cosine similarity\".\n",
        "\n",
        "We discussed above that if two embeddings are close - they are semantically similar, cosine similarity gives us a quick way to measure how similar two vectors are!\n",
        "\n",
        "Closeness is measured from 1 to -1, with 1 being extremely close and -1 being extremely close to opposite in meaning.\n",
        "\n",
        "Let's implement it with Numpy below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EVnhmxF4iD7P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(vec_1, vec_2):\n",
        "  return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okr5PMwBpjYE"
      },
      "source": [
        "We're going to be using OpenAI's `text-embedding-3-small` today.\n",
        "\n",
        "In order to choose our embeddings model, we'll refer to the MTEB leaberboard - which can be found [here](https://huggingface.co/spaces/mteb/leaderboard)!\n",
        "\n",
        "The basic logic is: We sort by our desired task - in this case `Retrieval Average (15 Datasets)`, and we're going to pick a model that performs well on that task - to keep cost in mind, we'll go with the `text-embedding-3-small` over the `text-embedding-3-large` since there's only a separation of ~5 points between the two on this task - but the cost is a significant factor less for the `small` version of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "quNjOLWspOVN"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3Qpbs2pxj_"
      },
      "source": [
        "Let's grab some vectors and see how they're related!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "C_uPM1llpSVt"
      },
      "outputs": [],
      "source": [
        "puppy_vec = embedding_model.embed_query(\"puppy\")\n",
        "dog_vec = embedding_model.embed_query(\"dog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OMzg_Pyp4qV"
      },
      "source": [
        "Let's do a quick check to ensure they're all the correct dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsGZ92hm9IeX"
      },
      "source": [
        "####â“ Question #2:\n",
        "\n",
        "What is the embedding dimension, given that we're using `text-embedding-3-small`?\n",
        "\n",
        "> HINT: Check out the [docs](https://platform.openai.com/docs/guides/embeddings) to help you answer this question.\n",
        "\n",
        "#### ANSWER: 1536"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIl5RQ98qFVu"
      },
      "source": [
        "Now, let's see how \"puppy\" and \"dog\" are related to eachother!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVn0WjMNqJrD",
        "outputId": "fb331f4c-b942-4862-aec7-709edf70c027"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5590390640733377"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(puppy_vec, dog_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfozbFmTielw"
      },
      "source": [
        "We can repeat the experiment for things we might expect to be unrelated, as well:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8PO-ONaJikDX"
      },
      "outputs": [],
      "source": [
        "puppy_vec = embedding_model.embed_query(\"puppy\")\n",
        "ice_vec = embedding_model.embed_query(\"ice cube\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-BYB5-WipM0",
        "outputId": "12372d9a-982a-470b-dd61-fa5dc4bf113d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.20365601127332958"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(puppy_vec, ice_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-AweaFtisKS"
      },
      "source": [
        "As expected, we get an unrelated score!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG9MnpJSsSUg"
      },
      "source": [
        "Great!\n",
        "\n",
        "Now, let's extend it to our example.\n",
        "\n",
        "What we want to do is find the most related phrases to our query - so what we need to do is find the dense continuous vector representations for each of the chunks in our courpus - and then compare them against the dense continuous vector representations of our query.\n",
        "\n",
        "In simpler terms:\n",
        "\n",
        "Compare the embedding of our query with the embeddings of each of our chunks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByK-zb0FsnqR"
      },
      "source": [
        "### Finding the Embeddings for Our Chunks\n",
        "\n",
        "First, let's find all our embeddings for each chunk and store them in a convenient format for later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZHl-u6Fxske9"
      },
      "outputs": [],
      "source": [
        "embeddings_dict = {}\n",
        "\n",
        "for chunk in chunks:\n",
        "  embeddings_dict[chunk] = embedding_model.embed_query(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhJ9wY2etK7y",
        "outputId": "ab4b35c9-5a70-477d-96fc-49bf8ee8e11d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk - LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
            "---\n",
            "Embedding - Vector of Size: 1536\n",
            "\n",
            "\n",
            "\n",
            "Chunk - Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
            "\n",
            "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
            "---\n",
            "Embedding - Vector of Size: 1536\n",
            "\n",
            "\n",
            "\n",
            "Chunk - Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
            "---\n",
            "Embedding - Vector of Size: 1536\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for k,v in embeddings_dict.items():\n",
        "  print(f\"Chunk - {k}\")\n",
        "  print(\"---\")\n",
        "  print(f\"Embedding - Vector of Size: {len(v)}\")\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOxYybdNtkWv"
      },
      "source": [
        "Okay, great. Let's create a query - and then embed it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQD5Zwl1tLrZ",
        "outputId": "bfa56ab9-ee1d-4748-b806-a43a92c33ba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector of Size: 1536\n"
          ]
        }
      ],
      "source": [
        "query = \"Can LCEL help take code from the notebook to production?\"\n",
        "\n",
        "query_vector = embedding_model.embed_query(query)\n",
        "print(f\"Vector of Size: {len(query_vector)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkJhyvgEt5Vt"
      },
      "source": [
        "Now, let's compare it against each existing chunk's embedding by using cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwnJ0uYQt-G_",
        "outputId": "d2971742-4f79-4abe-c20b-b40701d69d16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
            "0.5372562043448947\n"
          ]
        }
      ],
      "source": [
        "max_similarity = -float('inf')\n",
        "closest_chunk = \"\"\n",
        "\n",
        "for chunk, chunk_vector in embeddings_dict.items():\n",
        "  cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
        "\n",
        "  if cosine_similarity_score > max_similarity:\n",
        "    closest_chunk = chunk\n",
        "    max_similarity = cosine_similarity_score\n",
        "\n",
        "print(closest_chunk)\n",
        "print(max_similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDC7HS7iumN-"
      },
      "source": [
        "And we get the expected result, which is the passage that specifically mentions prototyping in a Jupyter Notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPJexL1kuw45"
      },
      "source": [
        "### Creating a Retriever\n",
        "\n",
        "Now that we have an idea of how we're getting our most relevant information - let's see how we could create a pipeline that would automatically extract the closest chunk to our query and use it as context for our prompt!\n",
        "\n",
        "First, we'll wrap the above in a helper function!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tnLpo26pu8-1"
      },
      "outputs": [],
      "source": [
        "def retrieve_context(query, embeddings_dict, embedding_model):\n",
        "  query_vector = embedding_model.embed_query(query)\n",
        "  max_similarity = -float('inf')\n",
        "  closest_chunk = \"\"\n",
        "\n",
        "  for chunk, chunk_vector in embeddings_dict.items():\n",
        "    cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
        "\n",
        "    if cosine_similarity_score > max_similarity:\n",
        "      closest_chunk = chunk\n",
        "      max_similarity = cosine_similarity_score\n",
        "\n",
        "  return closest_chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytrINkVrvL1Q"
      },
      "source": [
        "Now, let's add it to our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Q26pl1osvNmL"
      },
      "outputs": [],
      "source": [
        "def simple_rag(query, embeddings_dict, embedding_model, chat_chain):\n",
        "  context = retrieve_context(query, embeddings_dict, embedding_model)\n",
        "\n",
        "  response = chat_chain.invoke({\"query\" : query, \"context\" : context})\n",
        "\n",
        "  return_package = {\n",
        "      \"query\" : query,\n",
        "      \"response\" : response,\n",
        "      \"retriever_context\" : context\n",
        "  }\n",
        "\n",
        "  return return_package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHjbWxTAvtLS",
        "outputId": "6801e6ba-8413-4a86-c736-9a359163081f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'What does LCEL do that makes it more reliable at scale?  Explain your reasoning.  Your audience is technical.',\n",
              " 'response': AIMessage(content='LCEL provides full sync, async, batch, and streaming support for any chain constructed using this language. This means that chains built with LCEL are inherently designed to handle various types of data processing scenarios, including those that require scalability. By automatically incorporating these features, LCEL reduces the likelihood of errors and performance issues when scaling up the chain to handle larger datasets or more complex operations. This reliability at scale is crucial for ensuring the smooth and efficient functioning of data processing pipelines in production environments.', response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 164, 'total_tokens': 262}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-2cc4e459-d8d9-4cc7-8aec-f199fe1c75a5-0'),\n",
              " 'retriever_context': 'LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\\n\\nAsync, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.'}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simple_rag(\"What does LCEL do that makes it more reliable at scale?  Explain your reasoning.  Your audience is technical.\", embeddings_dict, embedding_model, chat_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD2URVX3O3XJ"
      },
      "source": [
        "####â“ Question #3:\n",
        "\n",
        "What does LCEL do that makes it more reliable at scale?\n",
        "\n",
        "> HINT: Use your newly created `simple_rag` to help you answer this question!\n",
        "\n",
        "#### ANSWER:\n",
        "LCEL provides full sync, async, batch, and streaming support for any chain constructed using this language. This means that chains built with LCEL are inherently designed to handle various types of data processing scenarios, including those that require scalability. By automatically incorporating these features, LCEL reduces the likelihood of errors and performance issues when scaling up the chain to handle larger datasets or more complex operations. This reliability at scale is crucial for ensuring the smooth and efficient functioning of data processing pipelines in production environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfYsBu45-0pL"
      },
      "source": [
        "# ðŸ¤ Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2AOb-QHwJm"
      },
      "source": [
        "## Task #3: Create a Simple RAG Application Using Qdrant, OpenAI, and LCEL\n",
        "\n",
        "Now that we have a grasp on how LCEL works, and how we can use LangChain and OpenAI to interact with our data - let's step it up a notch and incorporate Qdrant!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJB3IxEwpnh"
      },
      "source": [
        "## LangChain Powered RAG\n",
        "\n",
        "First and foremost, LangChain provides a convenient way to store our chunks and their embeddings.\n",
        "\n",
        "It's called a `VectorStore`!\n",
        "\n",
        "We'll be using Drant as our `VectorStore` today. You can read more about it [here](https://qdrant.tech/documentation/).\n",
        "\n",
        "Think of a `VectorStore` as a smart way to house your chunks and their associated embedding vectors. The implementation of the `VectorStore` also allows for smarter and more efficient search of our embedding vectors - as the method we used above would not scale well as we got into the millions of chunks.\n",
        "\n",
        "Otherwise, the process remains relatively similar under the hood!\n",
        "\n",
        "Let's use [The Ultimate Hitchhiker's Guide](https://jaydixit.com/files/PDFs/TheultimateHitchhikersGuide.pdf) as our data today!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBErqPRgxgZR"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We'll be leveraging the `PyMUPDFLoader` to load our PDF directly from the web!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-AHA9L3Jxo3r"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "docs = PyMuPDFLoader(\"https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf\").load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH7ZPVJLx6Cn"
      },
      "source": [
        "### Chunking Our Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSCRQUSyBKT"
      },
      "source": [
        "Let's do the same process as we did before with our `RecursiveCharacterTextSplitter` - but this time we'll use ~200 tokens as our max chunk size!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "SzolG1FLx2f_"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 200,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpV4f1eXyXVJ",
        "outputId": "eec7df70-56d4-433a-a870-ad143c6f0a6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "517"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJ60Ck6ybe_"
      },
      "source": [
        "Alright, now we have 516 ~200 token long documents.\n",
        "\n",
        "Let's verify the process worked as intended by checking our max document length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "950mB338yZR8",
        "outputId": "91f274c3-1117-43ad-f6af-03bf4974e776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "189\n"
          ]
        }
      ],
      "source": [
        "max_chunk_length = 0\n",
        "\n",
        "for chunk in split_chunks:\n",
        "  max_chunk_length = max(max_chunk_length, tiktoken_len(chunk.page_content))\n",
        "\n",
        "print(max_chunk_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDt3RufQy1cP"
      },
      "source": [
        "Perfect! Now we can carry on to creating and storing our embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kocCe4zLy5qT"
      },
      "source": [
        "### Embeddings and Vector Storage\n",
        "\n",
        "We'll use the `text-embedding-3-small` embedding model again - and `Qdrant` to store all our embedding vectors for easy retrieval later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "7M0X1eVlWPFf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    split_chunks,\n",
        "    embedding_model,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Hitchiker's Guide\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NDvjfzXhVp"
      },
      "source": [
        "Now let's set up our retriever, just as we saw before, but this time using LangChain's simple `as_retriever()` method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Edjx19YBXavZ"
      },
      "outputs": [],
      "source": [
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1OM0DiYcOj-"
      },
      "source": [
        "#### Back to the Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7apXaEBzQai"
      },
      "source": [
        "We're ready to move to the next step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMhcU37dzV5k"
      },
      "source": [
        "### Setting up our RAG\n",
        "\n",
        "We'll use the LCEL we touched on earlier to create a RAG chain.\n",
        "\n",
        "Let's think through each part:\n",
        "\n",
        "1. First we need to retrieve context\n",
        "2. We need to pipe that context to our model\n",
        "3. We need to parse that output\n",
        "\n",
        "Let's start by setting up our prompt again, just so it's fresh in our minds!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oatgDa7cOXWV"
      },
      "source": [
        "####ðŸ—ï¸ Activity #2:\n",
        "\n",
        "Complete the prompt so that your RAG application answers queries based on the context provided, but *does not* answer queries if the context is unrelated to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "TE5tick_YPJj"
      },
      "outputs": [],
      "source": [
        "RAG_PROMPT = \"\"\"\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{question}\n",
        "\n",
        "your primary mission is to provide precise and contextually relevant answers to queries posed by users. For each query, you must diligently analyze the provided context to determine if it holds the necessary information pertinent to the query. Should the query align with the context, you are to retrieve the appropriate knowledge and generate a concise, accurate response. However, if the query does not pertain to the context given or if the context lacks sufficient information to formulate a reliable answer, you must gracefully decline to respond, indicating that the query falls outside the scope of the provided context. Your responses should uphold the principles of relevance and accuracy, ensuring each answer serves the user's need for specific and contextual information.  If you don't know the answer, respond I DO NOT KNOW THE ANSWER.\"\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZKadufhc2RL"
      },
      "source": [
        "#### Our RAG Chain\n",
        "\n",
        "Notice how we have a bit of a more complex chain this time - that's because we want to return our sources with the response.\n",
        "\n",
        "Let's break down the chain step-by-step:\n",
        "\n",
        "1. We invoke the chain with the `question` item. Notice how we only need to provide `question` since both the retreiver and the `\"question\"` object depend on it.\n",
        "  - We also chain our `\"question\"` into our `retriever`! This is what ultimately collects the context through Qdrant.\n",
        "2. We assign our collected context to a `RunnablePassthrough()` from the previous object. This is going to let us simply pass it through to the next step, but still allow us to run that section of the chain.\n",
        "3. We finally collect our response by chaining our prompt, which expects both a `\"question\"` and `\"context\"`, into our `llm`. We also, collect the `\"context\"` again so we can output it in the final response object.\n",
        "\n",
        "The key thing to keep in mind here is that we need to pass our context through *after* we've retrieved it - to populate the object in a way that doesn't require us to call it or try and use it for something else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "VnGthXpzzo-R"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | openai_chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0KAPrtFMtRd"
      },
      "source": [
        "Let's get a visual understanding of our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceq3JfVLM74-",
        "outputId": "9a220163-05bb-4cb0-a620-66ad6fe1d04c"
      },
      "outputs": [],
      "source": [
        "!pip install -qU grandalf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ocIXNGMsue",
        "outputId": "9e51c77e-e4d1-41f0-afd8-c6f3d598c61d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                       +---------------------------------+                         \n",
            "                       | Parallel<context,question>Input |                         \n",
            "                       +---------------------------------+                         \n",
            "                           *****                   ****                            \n",
            "                        ***                            ****                        \n",
            "                     ***                                   ****                    \n",
            "+--------------------------------+                             **                  \n",
            "| Lambda(itemgetter('question')) |                              *                  \n",
            "+--------------------------------+                              *                  \n",
            "                 *                                              *                  \n",
            "                 *                                              *                  \n",
            "                 *                                              *                  \n",
            "     +----------------------+                   +--------------------------------+ \n",
            "     | VectorStoreRetriever |                   | Lambda(itemgetter('question')) | \n",
            "     +----------------------+                   +--------------------------------+ \n",
            "                           *****                   *****                           \n",
            "                                ***             ***                                \n",
            "                                   ***       ***                                   \n",
            "                       +----------------------------------+                        \n",
            "                       | Parallel<context,question>Output |                        \n",
            "                       +----------------------------------+                        \n",
            "                                         *                                         \n",
            "                                         *                                         \n",
            "                                         *                                         \n",
            "                            +------------------------+                             \n",
            "                            | Parallel<context>Input |                             \n",
            "                            +------------------------+                             \n",
            "                              ****               ****                              \n",
            "                           ***                       ***                           \n",
            "                         **                             **                         \n",
            "     +-------------------------------+              +-------------+                \n",
            "     | Lambda(itemgetter('context')) |              | Passthrough |                \n",
            "     +-------------------------------+              +-------------+                \n",
            "                              ****               ****                              \n",
            "                                  ***         ***                                  \n",
            "                                     **     **                                     \n",
            "                           +-------------------------+                             \n",
            "                           | Parallel<context>Output |                             \n",
            "                           +-------------------------+                             \n",
            "                                         *                                         \n",
            "                                         *                                         \n",
            "                                         *                                         \n",
            "                       +---------------------------------+                         \n",
            "                       | Parallel<response,context>Input |                         \n",
            "                       +---------------------------------+                         \n",
            "                             ***                  ****                             \n",
            "                         ****                         ***                          \n",
            "                       **                                ****                      \n",
            "         +--------------------+                              **                    \n",
            "         | ChatPromptTemplate |                               *                    \n",
            "         +--------------------+                               *                    \n",
            "                    *                                         *                    \n",
            "                    *                                         *                    \n",
            "                    *                                         *                    \n",
            "             +------------+                  +-------------------------------+     \n",
            "             | ChatOpenAI |                  | Lambda(itemgetter('context')) |     \n",
            "             +------------+**                +-------------------------------+     \n",
            "                             ***                  ***                              \n",
            "                                ****          ****                                 \n",
            "                                    **      **                                     \n",
            "                       +----------------------------------+                        \n",
            "                       | Parallel<response,context>Output |                        \n",
            "                       +----------------------------------+                        \n"
          ]
        }
      ],
      "source": [
        "print(retrieval_augmented_qa_chain.get_graph().draw_ascii())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bQVzN_eX1M2"
      },
      "source": [
        "Let's try another visual representation:\n",
        "\n",
        "![image](https://i.imgur.com/Ad31AhL.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0C5CFRHOxtB"
      },
      "source": [
        "Let's test our chain out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "JSDyVefDaue4"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the significance of towels in Douglas Adam's Hitchhicker's Guide?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "yfEAoG3HLC3J",
        "outputId": "ecd0a464-c76b-4723-e807-a533ef6b1dc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The significance of towels in Douglas Adams\\' Hitchhiker\\'s Guide to the Galaxy is that a towel is described as \"about the most massively useful thing\" one can have. The book mentions that a towel has practical uses like being a distress signal, drying oneself off, and also holds immense psychological value. If a hitchhiker is seen with a towel, others may assume they are well-prepared and resourceful, leading to them being perceived as someone to be respected.'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSZFdCM5LFoq",
        "outputId": "9a853f7c-65d8-4772-e54c-d3e6a2ccf526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context:\n",
            "page_content=\"28  /  D O U G L A S  A D A M S  \\nthis device was in fact that most remarkable of all books ever to \\ncome out of the great publishing corporations of Ursa Minor - \\nThe Hitch Hiker's Guide to the Galaxy.  The reason why it was \\npublished in the form of a micro sub meson electronic \\ncomponent is that if it were printed in normal book form, an \\ninterstellar hitch hiker would require several inconveniently \\nlarge buildings to carry it around in. \\nBeneath that in Ford Prefect's satchel were a few biros, a \\nnotepad, and a largish bath towel from Marks and Spencer. \\nThe Hitch Hiker's Guide to the Galaxy has a few things to say on \\nthe subject of towels. \\nA towel, it says, is about the most massively useful thing an\" metadata={'source': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'file_path': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'page': 27, 'total_pages': 227, 'format': 'PDF 1.3', 'title': \"Hitchhiker's Guide to the Galaxy\", 'author': 'Douglas Adams', 'subject': 'document version 1.0', 'keywords': 'hanomag <01name@iname.com>', 'creator': \"Hitchhiker's Guide to the Galaxy.doc (Preview) - Microsoft Word\", 'producer': 'Acrobat PDFWriter 4.0 for Windows NT', 'creationDate': 'D:20010213123949', 'modDate': \"D:20010213124359-08'00'\", 'trapped': '', 'encryption': 'Standard V1 R2 40-bit RC4', '_id': 'e42a31aea9c345d081c065c955441b9d', '_collection_name': \"Hitchiker's Guide\"}\n",
            "----\n",
            "Context:\n",
            "page_content=\"you can't see it, it can't see you - daft as a bush, but very \\nravenous); you can wave your towel in emergencies as a distress \\nsignal, and of course dry yourself off with it if it still seems to be \\nclean enough. \\nMore importantly, a towel has immense psychological value.  For \\nsome reason, if a strag (strag: non-hitch hiker) discovers that a \\nhitch hiker has his towel with him, he will automatically assume \\nthat he is also in possession of a toothbrush, face flannel, soap, tin\" metadata={'source': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'file_path': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'page': 27, 'total_pages': 227, 'format': 'PDF 1.3', 'title': \"Hitchhiker's Guide to the Galaxy\", 'author': 'Douglas Adams', 'subject': 'document version 1.0', 'keywords': 'hanomag <01name@iname.com>', 'creator': \"Hitchhiker's Guide to the Galaxy.doc (Preview) - Microsoft Word\", 'producer': 'Acrobat PDFWriter 4.0 for Windows NT', 'creationDate': 'D:20010213123949', 'modDate': \"D:20010213124359-08'00'\", 'trapped': '', 'encryption': 'Standard V1 R2 40-bit RC4', '_id': '39f4e5e262504a42a4aa85fa7d4cde71', '_collection_name': \"Hitchiker's Guide\"}\n",
            "----\n",
            "Context:\n",
            "page_content='T H E  H I T C H H I K E R \\' S  G U I D E  T O  T H E  G A L A X Y  /  29  \\nof biscuits, flask, compass, map, ball of string, gnat spray, wet \\nweather gear, space suit etc., etc.  Furthermore, the strag will \\nthen happily lend the hitch hiker any of these or a dozen other \\nitems that the hitch hiker might accidentally have \"lost\".  What \\nthe strag will think is that any man who can hitch the length and \\nbreadth of the galaxy, rough it, slum it, struggle against terrible \\nodds, win through, and still knows where his towel is is clearly a \\nman to be reckoned with. \\nHence a phrase which has passed into hitch hiking slang, as in' metadata={'source': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'file_path': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'page': 28, 'total_pages': 227, 'format': 'PDF 1.3', 'title': \"Hitchhiker's Guide to the Galaxy\", 'author': 'Douglas Adams', 'subject': 'document version 1.0', 'keywords': 'hanomag <01name@iname.com>', 'creator': \"Hitchhiker's Guide to the Galaxy.doc (Preview) - Microsoft Word\", 'producer': 'Acrobat PDFWriter 4.0 for Windows NT', 'creationDate': 'D:20010213123949', 'modDate': \"D:20010213124359-08'00'\", 'trapped': '', 'encryption': 'Standard V1 R2 40-bit RC4', '_id': 'fff040555e0a426bb576cbaed5b45e34', '_collection_name': \"Hitchiker's Guide\"}\n",
            "----\n",
            "Context:\n",
            "page_content='\"Hey, you sass that hoopy Ford Prefect? There\\'s a frood who \\nreally knows where his towel is.\" (Sass: know, be aware of, meet, \\nhave sex with; hoopy: really together guy; frood: really amazingly \\ntogether guy.) \\nNestling quietly on top of the towel in Ford Prefect\\'s satchel, the \\nSub-Etha Sens-O-Matic began to wink more quickly.  Miles above \\nthe surface of the planet the huge yellow somethings began to fan \\nout.  At Jodrell Bank, someone decided it was time for a nice \\nrelaxing cup of tea. \\n\"You got a towel with you?\" said Ford Prefect suddenly to Arthur. \\nArthur, struggling through his third pint, looked round at him. \\n\"Why? What, no ...  should I have?\" He had given up being' metadata={'source': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'file_path': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'page': 28, 'total_pages': 227, 'format': 'PDF 1.3', 'title': \"Hitchhiker's Guide to the Galaxy\", 'author': 'Douglas Adams', 'subject': 'document version 1.0', 'keywords': 'hanomag <01name@iname.com>', 'creator': \"Hitchhiker's Guide to the Galaxy.doc (Preview) - Microsoft Word\", 'producer': 'Acrobat PDFWriter 4.0 for Windows NT', 'creationDate': 'D:20010213123949', 'modDate': \"D:20010213124359-08'00'\", 'trapped': '', 'encryption': 'Standard V1 R2 40-bit RC4', '_id': '6c7a67c27d99405eb35d8e82e68b5883', '_collection_name': \"Hitchiker's Guide\"}\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "for context in response[\"context\"]:\n",
        "  print(\"Context:\")\n",
        "  print(context)\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nagiJ6l6noPL"
      },
      "source": [
        "Let's see if it can handle a query that is totally unrelated to the source documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "HOd2nJKZnsty"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the airspeed velocity of an unladen swallow?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TmLCKNGZLTh6",
        "outputId": "a68a9a96-28de-408f-ba6b-935c5ee46c44"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I don't know the answer to that question based on the provided context.\""
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Arthur Dent's house is about to be knocked down by bulldozers unless he lies in front of them to prevent it from happening.\""
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\": \"What is about to happen to Arthur Dent's house?\"})\n",
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvkSiGXrPMYw"
      },
      "source": [
        "####â“ Question #4:\n",
        "\n",
        "Where does Arthur Dent meet Marvin?\n",
        "\n",
        "> HINT: Use your RAG Chain to answer this question.\n",
        "\n",
        "#### ANSWER:\n",
        "'Arthur Dent meets Marvin in a corridor while Marvin is complaining about the pain in his diodes.'\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
